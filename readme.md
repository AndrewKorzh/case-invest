# Case

Файл в корневой папке проекта
`config.py:`

`db_config` = {   
    "dbname": "xxx",   
    "user": "xxx",     
    "password": "xxx",   
    "host": "xxx",   
    "port": "xxx",    
}

`ARCHIVE_PATH` = "xxx"

`STAGE_SCHEMA_NAME` = "xxx"

`stage_filler.py` - заполнение данными
`stage_former.py` - создание всех таблиц

---

`docker run --name my-postgres -e POSTGRES_PASSWORD=mysecretpassword -d -p 5432:5432 postgres` - простой способ развернуть postgres локально с помощью docker
# Этапы  
Отдельный модуль для создания stage (stage_former.py)
- Нужен для первичной настройки - просто пустые таблички со связями + триггеры + таблицы с логгированием ошибок

Заполнение данными (stage_filler.py)
- цикл по всем табличкам, для каждой процесс идентичный
- В data_structure получили пару ключ-значение - в ней необходимые данные о конкретной табличке
- Проходимся по всем файлам csv в папке (имя указано, путь к архиву черех config) и проверяем (только по первой строчке - быстрый процесс) наличие заголовков, соответствие ожидаемой структуре

| file_path | success | headers |
|----------------|----------------|----------------|
| ...file1.csv    | True     | True     |
| ...file2.csv     | True     | False     |
| ...file3.csv     | False     | False     |

- Создаём временную таблицу, где все столбцы text (без индексов и тд) - она нужна для обработки

- Все успешные файлы через \copy во временную таблицу, неуспешные - записываем в служебную БД
 
- Далее производим вставку данных и удаляем таблицу

# Реестр проверок
- PK - уникальные
- "" -> NULL
- Дата конца не больше даты начала...

# Проверка качества - в витрины
- Сколько записей пропущено
- Какие файлы не были загружены
- Заполненность полей по каждой табличке (можно в виде графика)
- Недопустимые значения - зп < 0 и тд


# Очерёдность
- Создание всех таблиц
- 
# TODO
- Обернуть всё что можно в try exept
- У меня если файл не проходит - просто пропуск, надо же сделать чтобы записывалось и как-то учитывался его размер
- Сделать единый файл с данными о табличках - из него уже генерить всё, можно из data_structure + переписать и сделать как массив, тогда не надо будет париться с очерёдностью (чтобы точно, хоть и словари в python упорядоченные) - соответественно - добавить fk везде
- Время ошибки + очищать иногда